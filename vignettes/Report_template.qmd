---
title: "Report template for (insert name of data or study)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Report_demo_1}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

### Statement of the problem from the customer's perspective

### History of the problem, previous results

# Exploratory data analysis

Head of the data
  Insert head of data here

Discuss the characteristics of each feature in the original data set

y (predictor) vs target variables (insert plot here)
Discussion of y vs target variables

Boxplots of the numeric data (insert plot here)
Discussion of boxplots of the numeric data

Histograms of each numeric column (insert plot here)
Discussion of histograms of each numeric column

Data summary (insert table here)
Discussion of the data summary

Outliers in the data (insert outliers data here)
Discussion of outliers in the data

The stories in the exploratory data analysis

# 32 Numeric models (18 individual and 14 ensembles)

One paragrapgh summary about statistical modeling

1. Bagging:
bagging_train_fit <- ipred::bagging(formula = y ~ ., data = train)

2. BayesGLM:
bayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = gaussian(link = "identity"))

3. BayesRNN
bayesrnn_train_fit <- brnn::brnn(x = as.matrix(train), y = train$y)

4. Cubist
cubist_train_fit <- Cubist::cubist(x = train[, 1:ncol(train) - 1], y = train$y)

5. Earth
earth_train_fit <- earth::earth(x = train[, 1:ncol(train) - 1], y = train$y)

6. Elastic (optimized by cross-validation)
best_elastic_model <- glmnet::glmnet(x, y, alpha = 0, lambda = best_elastic_lambda)

7. Ensemble Bagging
ensemble_bagging_train_fit <- ipred::bagging(formula = y_ensemble ~ ., data = ensemble_train)

8. Ensemble BayesGLM
ensemble_bayesglm_train_fit <- arm::bayesglm(y_ensemble ~ ., data = ensemble_train, family = gaussian(link = "identity"))

9. Ensemble BayesRNN
ensemble_bayesrnn_train_fit <- brnn::brnn(x = as.matrix(ensemble_train), y = ensemble_train$y_ensemble)

10. Ensemble Cubist
ensemble_cubist_train_fit <- Cubist::cubist(x = ensemble_train[, 1:ncol(ensemble_train) - 1], y = ensemble_train$y_ensemble)

11. Ensemble Earth
ensemble_earth_train_fit <- earth::earth(x = ensemble_train[, 1:ncol(ensemble_train) - 1], y = ensemble_train$y_ensemble)

12. Ensemble Elastic (optimized by cross-validation)
ensemble_best_elastic_model <- glmnet(ensemble_x, ensemble_y, alpha = 0, lambda = ensemble_best_elastic_lambda)

13. Ensemble Gradient Boosted
ensemble_gb_train_fit <- gbm::gbm(ensemble_train$y_ensemble ~ .,
                                        data = ensemble_train, distribution = "gaussian", n.trees = 100,
                                        shrinkage = 0.1, interaction.depth = 10

14. Ensemble Lasso (optimized by cross-validation)
ensemble_best_lasso_model <- glmnet(ensemble_x, ensemble_y, alpha = 1, lambda = ensemble_best_lasso_lambda)

15. Ensemble Linear (tuned)
ensemble_linear_train_fit <- e1071::tune.rpart(formula = y_ensemble ~ ., data = ensemble_train)

16. Ensemble Ridge (optimized by cross-validation)
ensemble_best_ridge_model <- glmnet(ensemble_x, ensemble_y, alpha = 0, lambda = ensemble_best_ridge_lambda)

17. Ensemble RPart
ensemble_rpart_train_fit <- rpart::rpart(ensemble_train$y_ensemble ~ ., data = ensemble_train)

18. EnsembleSVM (tuned)
ensemble_svm_train_fit <- e1071::tune.svm(x = ensemble_train, y = ensemble_train$y_ensemble, data = ensemble_train)

19. Ensemble Trees
ensemble_tree_train_fit <- tree::tree(ensemble_train$y_ensemble ~ ., data = ensemble_train)

20. Ensemble XGBoost
ensemble_xgb_model <- xgboost::xgb.train(data = ensemble_xgb_train, max.depth = 3, watchlist = ensemble_watchlist_test, nrounds = 70)
ensemble_xgb_model_validation <- xgboost::xgb.train(data = ensemble_xgb_train, max.depth = 3, watchlist = ensemble_watchlist_validation, nrounds = 70)

21. GAM (Generalized Additive Models, with smoothing splines)
gam_train_fit <- gam(f2, data = train1)

22. Gradient Boosted (optimized)
gb_train_fit <- gbm::gbm(train$y ~ ., data = train, distribution = "gaussian", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)

23. Lasso (optimized using cross-validation)
best_lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lasso_lambda)

24. Linear (tuned)
linear_train_fit <- e1071::tune.rpart(formula = y ~ ., data = train)

25. Neuralnet
 neuralnet_train_fit <- nnet::nnet(train$y ~ ., data = train, size = 0, linout = TRUE, skip = TRUE)

26. PCR (Principal Components Regression)
 pcr_train_fit <- pls::pcr(train$y ~ ., data = train)

27. PLS (Partial Least Squares)
pls_train_fit <- pls::plsr(train$y ~ ., data = train)

28. Ridge (optimized by cross-validation)
best_ridge_model <- glmnet(x, y, alpha = 0, lambda = best_ridge_lambda)

29. RPart
rpart_train_fit <- rpart::rpart(train$y ~ ., data = train)

30. SVM (Support Vector Machines, tuned)
svm_train_fit <- e1071::tune.svm(x = train, y = train$y, data = train)

31. Tree
tree_train_fit <- tree::tree(train$y ~ ., data = train)

32. XGBoost
xgb_model <- xgboost::xgb.train(data = xgb_train, max.depth = 3, watchlist = watchlist_test, nrounds = 70)
xgb_model_validation <- xgboost::xgb.train(data = xgb_train, max.depth = 3, watchlist = watchlist_validation, nrounds = 70)

# The most accurate model results

-   Most accurate model results:

    -   Predicted vs actual

        -   Discussion of most accurate predicted vs actual

    -   Residuals

        -   Discussion of residuals from the most accurate model

    -   Histogram of residuals

        -   Discussion of the histogram of residuals from the most accurate model

    -   Q-Q plot

        -   Discussion of the Q-Q plot from the most accurate model

    -   Mean accuracy for train, test, validation, and holdout (mean of test and validation) sets

    -   Barchart of model accuracy

        -   Measures mean Root Mean Squared Error, from low to high

        -   Includes 1 standard deviation error bars

    -   Insert model accuracy numbers with standard deviations

    -   Insert model accuracy bar chart with 1 standard deviation error bars here

-   Bias

    -   `bias` computes the average amount by which `actual` is greater than `predicted`.

    -   If a model is unbiased `bias(actual, predicted)` should be close to zero. Bias is calculated by taking the average of (`actual` - `predicted`).

    -   Insert bias plot here

-   Holdout / Train

    -   Calculates the mean of the holdout RMSE / mean of the train RMSE across the samples. Closer to one is better.

    -   Insert table for holdout / train numbers

    -   Insert plot for holdout / train chart here

-   Duration

    -   Calculates the mean duration for each model

    -   Insert barchart for duration here

-   t-test with p-values

    -   Performs one and two sample t-tests on vectors of data

    -   Insert t-test numbers

    -   Insert plot for t-test results

-   Barchart of Kolmogorov-Smirnov test by model

    -   stats::ks.test(x = y_hat_bag_rf, y = c(train$y, validation$y), exact = TRUE)\$statistic

    -   Tests if the holdout data from each of the 40 models came from the same distribution as the raw data. If `y` is numeric, a two-sample (Smirnov) test of the null hypothesis that `x` and `y` were drawn from the same distribution is performed.

    -   Lines are given for p = 0.05 and p = 0.10,

-   Variance Inflation Factor report

    -   Calculates variance-inflation and generalized variance-inflation factors (VIFs and GVIFs) for linear, generalized linear, and other regression models.

-   Correlation of the data

    -   Presents a table of the correlation of the data.

-   Correlation of the ensemble

    -   Presents a table of the correlation of the ensemble. This can be modified in the function call, such as remove_ensemble_correlations_greater_than = 0.98 (or whatever value is most useful)

-   Summary report

-   Function call

-   Warnings or errors

-   The stories in the plots

# Strongest evidence based data:

-   Most accurate models with error ranges

-   Strongest predictor with error ranges

-   The stories of the strongest evidenced based data

# Five strongest evidence based recommendations

# Conclusions
